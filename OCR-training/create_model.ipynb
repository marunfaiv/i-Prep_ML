{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ae5fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31eb9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_az_dataset(datasetPath):\n",
    "\t# initialize the list of data and labels\n",
    "\tdata = []\n",
    "\tlabels = []\n",
    "\t# loop over the rows of the A-Z handwritten digit dataset\n",
    "\tfor row in open(datasetPath):\n",
    "\t\t# parse the label and image from the row\n",
    "\t\trow = row.split(\",\")\n",
    "\t\tlabel = int(row[0])\n",
    "\t\timage = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "\t\t# images are represented as single channel (grayscale) images\n",
    "\t\t# that are 28x28=784 pixels -- we need to take this flattened\n",
    "\t\t# 784-d list of numbers and repshape them into a 28x28 matrix\n",
    "\t\timage = image.reshape((28, 28))\n",
    "\t\t# update the list of data and labels\n",
    "\t\tdata.append(image)\n",
    "\t\tlabels.append(label)\n",
    "    \n",
    "    # convert the data and labels to NumPy arrays\n",
    "\tdata = np.array(data, dtype=\"float32\")\n",
    "\tlabels = np.array(labels, dtype=\"int\")\n",
    "\t# return a 2-tuple of the A-Z data and labels\n",
    "\treturn (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb5a302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "\t# load the MNIST dataset and stack the training data and testing\n",
    "\t# data together (we'll create our own training and testing splits\n",
    "\t# later in the project)\n",
    "\t((trainData, trainLabels), (testData, testLabels)) = mnist.load_data()\n",
    "\tdata = np.vstack([trainData, testData])\n",
    "\tlabels = np.hstack([trainLabels, testLabels])\n",
    "\t# return a 2-tuple of the MNIST data and labels\n",
    "\treturn (data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae281b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a69ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading datasets...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-1\n",
    "BS = 128\n",
    "# load the A-Z and MNIST datasets, respectively\n",
    "print(\"[INFO] loading datasets...\")\n",
    "(azData, azLabels) = load_az_dataset('./Data/A_Z Handwritten Data.csv')\n",
    "(digitsData, digitsLabels) = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44d60608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the MNIST dataset occupies the labels 0-9, so let's add 10 to every\n",
    "# A-Z label to ensure the A-Z characters are not incorrectly labeled\n",
    "# as digits\n",
    "azLabels += 10\n",
    "# stack the A-Z data and labels with the MNIST digits data and labels\n",
    "data = np.vstack([azData, digitsData])\n",
    "labels = np.hstack([azLabels, digitsLabels])\n",
    "# each image in the A-Z and MNIST digts datasets are 28x28 pixels;\n",
    "# however, the architecture we're using is designed for 32x32 images,\n",
    "# so we need to resize them to 32x32\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "# add a channel dimension to every image in the dataset and scale the\n",
    "# pixel intensities of the images from [0, 255] down to [0, 1]\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "231c55e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels from integers to vectors\n",
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "# account for skew in the labeled data\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = {}\n",
    "# loop over all classes and calculate the class weight\n",
    "for i in range(0, len(classTotals)):\n",
    "\tclassWeight[i] = classTotals.max() / classTotals[i]\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93794ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=10,\n",
    "\tzoom_range=0.05,\n",
    "\twidth_shift_range=0.1,\n",
    "\theight_shift_range=0.1,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=False,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9af3f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\tif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "# \t\t# loop over the number of stages\n",
    "# \t\tfor i in range(0, len(stages)):\n",
    "# \t\t\t# initialize the stride, then apply a residual module\n",
    "# \t\t\t# used to reduce the spatial size of the input volume\n",
    "# \t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "# \t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "# \t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "# \t\t\t# loop over the number of layers in the stage\n",
    "# \t\t\tfor j in range(0, stages[i] - 1):\n",
    "# \t\t\t\t# apply a ResNet module\n",
    "# \t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "# \t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((4, 4))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49ce25bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# initialize and compile our deep neural network\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3),\n",
    "\t(64, 64, 128, 256), reg=0.0005, dataset='tiny_imagenet')\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "360fb2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 32, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        1600      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 32, 32, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 34, 34, 64)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 4, 4, 64)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 36)                36900     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 36)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,016\n",
      "Trainable params: 38,758\n",
      "Non-trainable params: 258\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ee10357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/12\n",
      "2765/2765 [==============================] - 511s 185ms/step - loss: 1.0859 - accuracy: 0.9140 - val_loss: 0.5967 - val_accuracy: 0.8462\n",
      "Epoch 2/12\n",
      "2765/2765 [==============================] - 523s 189ms/step - loss: 1.0631 - accuracy: 0.9165 - val_loss: 0.4415 - val_accuracy: 0.8934\n",
      "Epoch 3/12\n",
      "2765/2765 [==============================] - 527s 191ms/step - loss: 1.0454 - accuracy: 0.9173 - val_loss: 0.4529 - val_accuracy: 0.8919\n",
      "Epoch 4/12\n",
      "2765/2765 [==============================] - 525s 190ms/step - loss: 1.0288 - accuracy: 0.9183 - val_loss: 0.4755 - val_accuracy: 0.8842\n",
      "Epoch 5/12\n",
      "2765/2765 [==============================] - 528s 191ms/step - loss: 1.0135 - accuracy: 0.9200 - val_loss: 0.6224 - val_accuracy: 0.8463\n",
      "Epoch 6/12\n",
      "2765/2765 [==============================] - 528s 191ms/step - loss: 1.0059 - accuracy: 0.9208 - val_loss: 0.6340 - val_accuracy: 0.8444\n",
      "Epoch 7/12\n",
      "2765/2765 [==============================] - 527s 191ms/step - loss: 0.9942 - accuracy: 0.9213 - val_loss: 0.5027 - val_accuracy: 0.8809\n",
      "Epoch 8/12\n",
      "2765/2765 [==============================] - 528s 191ms/step - loss: 0.9849 - accuracy: 0.9216 - val_loss: 0.4866 - val_accuracy: 0.8838\n",
      "Epoch 9/12\n",
      "2765/2765 [==============================] - 549s 199ms/step - loss: 0.9808 - accuracy: 0.9224 - val_loss: 0.3331 - val_accuracy: 0.9326\n",
      "Epoch 10/12\n",
      "2765/2765 [==============================] - 529s 191ms/step - loss: 0.9802 - accuracy: 0.9224 - val_loss: 0.4668 - val_accuracy: 0.8900\n",
      "Epoch 11/12\n",
      "2765/2765 [==============================] - 533s 193ms/step - loss: 0.9678 - accuracy: 0.9230 - val_loss: 0.4262 - val_accuracy: 0.9035\n",
      "Epoch 12/12\n",
      "2765/2765 [==============================] - 537s 194ms/step - loss: 0.9714 - accuracy: 0.9231 - val_loss: 0.5985 - val_accuracy: 0.8510\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=12,\n",
    "\tclass_weight=classWeight,\n",
    "\tverbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce29a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('handwriting.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c18b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list of label names\n",
    "labelNames = \"0123456789\"\n",
    "labelNames += \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "labelNames = [l for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c83fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "692/692 [==============================] - 17s 24ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.96      0.33      1381\n",
      "           1       0.95      0.98      0.96      1575\n",
      "           2       0.88      0.96      0.92      1398\n",
      "           3       0.87      0.98      0.92      1428\n",
      "           4       0.88      0.94      0.91      1365\n",
      "           5       0.19      0.99      0.32      1263\n",
      "           6       0.96      0.97      0.96      1375\n",
      "           7       0.92      0.99      0.96      1459\n",
      "           8       0.86      0.98      0.92      1365\n",
      "           9       0.92      0.99      0.95      1392\n",
      "           A       0.99      0.96      0.97      2774\n",
      "           B       0.99      0.89      0.94      1734\n",
      "           C       0.98      0.98      0.98      4682\n",
      "           D       0.90      0.94      0.92      2027\n",
      "           E       1.00      0.94      0.97      2288\n",
      "           F       0.91      1.00      0.95       232\n",
      "           G       0.97      0.92      0.94      1152\n",
      "           H       0.96      0.94      0.95      1444\n",
      "           I       0.98      0.96      0.97       224\n",
      "           J       0.89      0.96      0.93      1699\n",
      "           K       0.94      0.96      0.95      1121\n",
      "           L       0.97      0.98      0.98      2317\n",
      "           M       0.97      0.99      0.98      2467\n",
      "           N       0.98      0.98      0.98      3802\n",
      "           O       0.99      0.54      0.70     11565\n",
      "           P       0.99      0.98      0.98      3868\n",
      "           Q       0.98      0.93      0.95      1162\n",
      "           R       0.98      0.96      0.97      2313\n",
      "           S       1.00      0.44      0.61      9684\n",
      "           T       1.00      0.97      0.98      4499\n",
      "           U       0.99      0.95      0.97      5802\n",
      "           V       0.98      1.00      0.99       836\n",
      "           W       0.97      0.98      0.97      2157\n",
      "           X       0.96      0.98      0.97      1254\n",
      "           Y       0.98      0.93      0.96      2172\n",
      "           Z       0.95      0.91      0.93      1215\n",
      "\n",
      "    accuracy                           0.85     88491\n",
      "   macro avg       0.91      0.94      0.90     88491\n",
      "weighted avg       0.95      0.85      0.87     88491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2e36782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network...\n"
     ]
    }
   ],
   "source": [
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save('handwriting.model', save_format=\"h5\")\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, 12)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0908de46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88491, 32, 32, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4011594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[np.newaxis, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b5a0ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our list of output test images\n",
    "images = []\n",
    "# randomly select a few testing characters\n",
    "for i in np.random.choice(np.arange(0, len(testY)), size=(100,)):\n",
    "\t# classify the character\n",
    "\tprobs = model.predict(testX[np.newaxis, i])\n",
    "\tprediction = probs.argmax(axis=1)\n",
    "\tlabel = labelNames[prediction[0]]\n",
    "\t# extract the image from the test data and initialize the text\n",
    "\t# label color as green (correct)\n",
    "\timage = (testX[i] * 255).astype(\"uint8\")\n",
    "\tcolor = (0, 255, 0)\n",
    "\t# otherwise, the class label prediction is incorrect\n",
    "\tif prediction[0] != np.argmax(testY[i]):\n",
    "\t\tcolor = (0, 0, 255)\n",
    "\t# merge the channels into one image, resize the image from 32x32\n",
    "\t# to 96x96 so we can better see it and then draw the predicted\n",
    "\t# label on the image\n",
    "\timage = cv2.merge([image] * 3)\n",
    "\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
    "\t\tcolor, 2)\n",
    "\t# add the image to our list of output images\n",
    "\timages.append(image)\n",
    "# construct the montage for the images\n",
    "montage = build_montages(images, (96, 96), (10, 10))[0]\n",
    "# show the output montage\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "777354a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('handwriting.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d29685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
